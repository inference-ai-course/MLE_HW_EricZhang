{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Assignment: Voice Agent Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we start to hands on buiding Research Voice Agent, truly useful AI Research Assistants must listen, understand, and respond with voice. **we will give you some simple introduction code as a starter, feel free to write your own code or do optimization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Learning Objectives this week\n",
    "to build a simple Voice Agent, we need these following knowledge.\n",
    "\n",
    "* **1. Speech Recognition (ASR):** Convert audio to text using models like Whisper or Google Speech-to-Text.\n",
    "* **2. Dialogue Generation with LLMs:** Feed transcribed user input into LLM (e.g. LLaMA 3) and generate natural language responses.\n",
    "* **3. Text-to-Speech (TTS):** Use a TTS engine (CozyVoice) to convert generated responses into spoken audio.\n",
    "* **4. FastAPI for API Serving:** Create a web server with FastAPI to handle audio file uploads and return voice responses.\n",
    "* **5. Conversation State Management:** Track conversation history to enable multi-turn interaction.\n",
    "* **6. Low-Latency Real-Time Processing:** Use asynchronous functions to reduce inference time and improve response experience.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> âœ… You do NOT need Docker. Just ensure your local Python environment works.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Project: Build an Local Voice Assistant\n",
    "\n",
    "### ðŸŽ¯ Goal:\n",
    "\n",
    "Develop a real-time voice chatbot that can:\n",
    "\n",
    "1. Take audio input via HTTP,\n",
    "2. Transcribe audio to text (ASR),\n",
    "3. Generate a response using LLM,\n",
    "4. Convert the response back to speech (TTS),\n",
    "5. Support 5-turn conversational memory.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: FastAPI Skeleton\n",
    "\n",
    "Create a simple FastAPI server that accepts an audio file via POST and returns an audio file in response:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is the official guidance of FastAPI [fastapi](https://fastapi.tiangolo.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: python-multipart\n",
      "Successfully installed python-multipart-0.0.20\n"
     ]
    }
   ],
   "source": [
    "!pip install python-multipart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import FileResponse\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/chat/\")\n",
    "async def chat_endpoint(file: UploadFile = File(...)):\n",
    "    audio_bytes = await file.read()\n",
    "    # TODO: ASR â†’ LLM â†’ TTS\n",
    "    return FileResponse(\"response.wav\", media_type=\"audio/wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your server:\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "Test it with `curl`, Postman, or a custom frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running uvicorn 0.35.0 with CPython 3.9.0 on Windows\n"
     ]
    }
   ],
   "source": [
    "!uvicorn --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['c:\\\\Users\\\\zhang\\\\OneDrive\\\\Documents\\\\Programming\\\\Inference AI Course\\\\MLE_HW_EricZhang\\\\Week 3']\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [69832] using StatReload\n",
      "INFO:     Started server process [75240]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "WARNING:  StatReload detected changes in 'main.py'. Reloading...\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [75240]\n",
      "Process SpawnProcess-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\zhang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"C:\\Users\\zhang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\users\\zhang\\onedrive\\documents\\programming\\inference ai course\\.venv\\lib\\site-packages\\uvicorn\\_subprocess.py\", line 80, in subprocess_started\n",
      "    target(sockets=sockets)\n",
      "  File \"c:\\users\\zhang\\onedrive\\documents\\programming\\inference ai course\\.venv\\lib\\site-packages\\uvicorn\\server.py\", line 67, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"C:\\Users\\zhang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"C:\\Users\\zhang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 642, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"c:\\users\\zhang\\onedrive\\documents\\programming\\inference ai course\\.venv\\lib\\site-packages\\uvicorn\\server.py\", line 71, in serve\n",
      "    await self._serve(sockets)\n",
      "  File \"c:\\users\\zhang\\onedrive\\documents\\programming\\inference ai course\\.venv\\lib\\site-packages\\uvicorn\\server.py\", line 78, in _serve\n",
      "    config.load()\n",
      "  File \"c:\\users\\zhang\\onedrive\\documents\\programming\\inference ai course\\.venv\\lib\\site-packages\\uvicorn\\config.py\", line 436, in load\n",
      "    self.loaded_app = import_from_string(self.app)\n",
      "  File \"c:\\users\\zhang\\onedrive\\documents\\programming\\inference ai course\\.venv\\lib\\site-packages\\uvicorn\\importer.py\", line 22, in import_from_string\n",
      "    raise exc from None\n",
      "  File \"c:\\users\\zhang\\onedrive\\documents\\programming\\inference ai course\\.venv\\lib\\site-packages\\uvicorn\\importer.py\", line 19, in import_from_string\n",
      "    module = importlib.import_module(module_str)\n",
      "  File \"C:\\Users\\zhang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 790, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\zhang\\OneDrive\\Documents\\Programming\\Inference AI Course\\MLE_HW_EricZhang\\Week 3\\main.py\", line 5, in <module>\n",
      "    from cozyvoice import CozyVoice\n",
      "ModuleNotFoundError: No module named 'cozyvoice'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "!uvicorn main:app --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: ASR (Speech Recognition)\n",
    "\n",
    "Use OpenAI Whisper to transcribe the uploaded audio to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwhisper\u001b[39;00m\n\u001b[0;32m      3\u001b[0m asr_model \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtranscribe_audio\u001b[39m(audio_bytes):\n",
      "File \u001b[1;32mc:\\Users\\zhang\\OneDrive\\Documents\\Programming\\Inference AI Course\\.venv\\lib\\site-packages\\whisper.py:69\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CAN_FALLOCATE:\n\u001b[0;32m     68\u001b[0m   libc_name \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_library(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m   libc \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlibc_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m   c_off64_t \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int64\n\u001b[0;32m     71\u001b[0m   c_off_t \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ctypes\\__init__.py:364\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnt\u001b[39;00m\n\u001b[0;32m    363\u001b[0m mode \u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39m_LOAD_LIBRARY_SEARCH_DEFAULT_DIRS\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39m_getfullpathname(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name)\n\u001b[0;32m    366\u001b[0m     mode \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39m_LOAD_LIBRARY_SEARCH_DLL_LOAD_DIR\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "asr_model = whisper.load_model(\"small\")\n",
    "\n",
    "def transcribe_audio(audio_bytes):\n",
    "    with open(\"temp.wav\", \"wb\") as f:\n",
    "        f.write(audio_bytes)\n",
    "    result = asr_model.transcribe(\"temp.wav\")\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add it to the `/chat/` route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = transcribe_audio(audio_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print `user_text` for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: Response Generation (LLM)\n",
    "\n",
    "Generate context-aware responses using Llama 3. Use HuggingFace `pipeline` to call LLaMA 3 or similar models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\"text-generation\", model=\"meta-llama/Llama-3-8B\")\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "def generate_response(user_text):\n",
    "    conversation_history.append({\"role\": \"user\", \"text\": user_text})\n",
    "    # Construct prompt from history\n",
    "    prompt = \"\"\n",
    "    for turn in conversation_history[-5:]:\n",
    "        prompt += f\"{turn['role']}: {turn['text']}\\n\"\n",
    "    outputs = llm(prompt, max_new_tokens=100)\n",
    "    bot_response = outputs[0][\"generated_text\"]\n",
    "    conversation_history.append({\"role\": \"assistant\", \"text\": bot_response})\n",
    "    return bot_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call in route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_text = generate_response(user_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4: TTS (Text to Speech)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert LLM text responses to natural-sounding speech. \\\n",
    "try to use cozyvoice to complete Text to Speech, here is the original project.\n",
    "[Cozyvoice](https://github.com/FunAudioLLM/CosyVoice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cozyvoice import CozyVoice\n",
    "\n",
    "tts_engine = CozyVoice()\n",
    "\n",
    "def synthesize_speech(text, filename=\"response.wav\"):\n",
    "    tts_engine.generate(text, output_file=filename)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use it in the route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythonaudio_path = synthesize_speech(bot_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 5: Full Integration\n",
    "\n",
    "Your final `/chat/` endpoint might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@app.post(\"/chat/\")\n",
    "async def chat_endpoint(file: UploadFile = File(...)):\n",
    "    audio_bytes = await file.read()\n",
    "    user_text = transcribe_audio(audio_bytes)\n",
    "    bot_text = generate_response(user_text)\n",
    "    audio_path = synthesize_speech(bot_text)\n",
    "    return FileResponse(audio_path, media_type=\"audio/wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## âœ… Deliverables\n",
    "\n",
    "* [ ] A runnable FastAPI project with `/chat/` endpoint\n",
    "* [ ] A working voice assistant that handles **5-turn** multi-round conversations\n",
    "* [ ] Code with clear structure and modular components (ASR, LLM, TTS)\n",
    "* [ ] A **2-minute screen recording** demo: record 5 turns of real-time interaction\n",
    "* [ ] Optional: Add conversation memory display, prompt formatting logic, async optimization\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ Extension Ideas (Optional)\n",
    "\n",
    "* Use `async` processing for parallel ASR/LLM/TTS.\n",
    "* Integrate a microphone frontend UI for live recording.\n",
    "* Add speaker identification or personalized voice response.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
