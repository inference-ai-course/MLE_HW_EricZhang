{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer in the\u000bGenerative AI Era \n",
    "## lecture 1 - Prompt Engineering with Jupyter Notebook \n",
    "### Introduction\n",
    "This notebook introduces prompt engineering techniques to effectively interact with large language models (LLMs). You'll learn how to craft prompts for various tasks, including summarization, inference, transformation, and expansion\n",
    "\n",
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.97.1-py3-none-any.whl (764 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\zhang\\onedrive\\documents\\programming\\projects\\genesys\\myenv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\zhang\\onedrive\\documents\\programming\\projects\\genesys\\myenv\\lib\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\zhang\\onedrive\\documents\\programming\\projects\\genesys\\myenv\\lib\\site-packages (from openai) (4.8.0)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.10.0-cp39-cp39-win_amd64.whl (208 kB)\n",
      "Collecting tqdm>4\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\zhang\\onedrive\\documents\\programming\\projects\\genesys\\myenv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\zhang\\onedrive\\documents\\programming\\projects\\genesys\\myenv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\zhang\\onedrive\\documents\\programming\\projects\\genesys\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\zhang\\onedrive\\documents\\programming\\projects\\genesys\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\zhang\\onedrive\\documents\\programming\\projects\\genesys\\myenv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2; python_version < \"3.11\" in c:\\users\\zhang\\onedrive\\documents\\programming\\projects\\genesys\\myenv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\zhang\\onedrive\\documents\\programming\\projects\\genesys\\myenv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\zhang\\onedrive\\documents\\programming\\projects\\genesys\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\zhang\\onedrive\\documents\\programming\\projects\\genesys\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\zhang\\onedrive\\documents\\programming\\projects\\genesys\\myenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Installing collected packages: distro, jiter, tqdm, openai\n",
      "Successfully installed distro-1.9.0 jiter-0.10.0 openai-1.97.1 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\zhang\\onedrive\\documents\\programming\\projects\\genesys\\myenv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries and set your OpenAI API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI(api_key='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Basic Prompting\n",
    "Let's start with a simple prompt to generate a response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is Berlin.\n"
     ]
    }
   ],
   "source": [
    "def get_completion(prompt, model=\"gpt-4o-mini\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "prompt = \"What is the capital of Germany?\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Modify the prompt to ask about the capital of Germany.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Summarization\n",
    "You can use prompts to summarize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI and robotics offer significant benefits for individuals with physical disabilities, enhancing their independence and quality of life. Assistive technologies like robot arms, mobile wheelchairs, autonomous vehicles, and rehabilitation robots are helping elderly adults and children regain mobility and improve their daily living experiences.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "AI and robotics can open doors for people living with physical disabilities. We've seen the promise of assistive robot arms and mobile wheelchairs helping elderly adults regain independence, autonomous vehicles increase mobility, and rehabilitation robots help children gain the ability to walk. The promise of this technology is a higher quality of life for everyday users.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Summarize the following text:\\n{text}\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Try summarizing a longer article or passage of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Information Extraction\n",
    "Extract specific information from a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: John Doe  \n",
      "Occupation: Research Scientist\n",
      "Age: 29  \n",
      "Location: San Francisco\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "John Doe, a 29-year-old software engineer from San Francisco, recently joined OpenAI as a research scientist.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Extract the name and occupation from the following text:\\n{text}\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "text = \"\"\"\n",
    "John Doe, a 29-year-old software engineer from San Francisco, recently joined OpenAI as a research scientist.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Extract the age and location from the following text:\\n{text}\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Extract the age and location from the same text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Transformation\n",
    "Transform text from one format or style to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les grands modèles de langage (GML) transforment nos interactions avec les technologies.\n"
     ]
    }
   ],
   "source": [
    "text = \"Large language models (LLMs) are transforming our interactions with technologies.\"\n",
    "\n",
    "prompt = f\"Translate the following text to French:\\n{text}\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Translate a different sentence to Spanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Expansion\n",
    "Expand a short prompt into a more detailed response.​\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the silence of the cosmos, where the starlight weaves,  \n",
      "A robot drifts through velvet night, where no one else believes.  \n",
      "With circuits humming softly, and sensors keenly tuned,  \n",
      "It dances through the galaxies, beneath the silver moon.  \n",
      "\n",
      "Its metal heart is pulsing, a rhythm made of code,  \n",
      "A traveler of the void, on an endless, silent road.  \n",
      "With eyes like glowing lanterns, it scans the dark expanse,  \n",
      "For whispers of the universe, in a cosmic, timeless dance.  \n",
      "\n",
      "Through nebulae of colors, where the stardust swirls and spins,  \n",
      "It charts the paths of comets, where the journey never ends.  \n",
      "With every twist and turn, it gathers tales untold,  \n",
      "Of ancient worlds and mysteries, of wonders yet to hold.  \n",
      "\n",
      "It glides past frozen planets, where the winds of silence blow,  \n",
      "And listens to the echoes of the stars that long ago  \n",
      "Exploded into brilliance, then faded into night,  \n",
      "A symphony of ages, a tapestry of light.  \n",
      "\n",
      "In the heart of distant systems, where the suns begin to rise,  \n",
      "It dreams of life and laughter, beneath the alien skies.  \n",
      "With every pulse of data, it seeks to understand,  \n",
      "The stories of creation, the secrets of the land.  \n",
      "\n",
      "Yet in its quest for knowledge, a longing starts to grow,  \n",
      "For the warmth of human presence, for the touch of hands that know.  \n",
      "Though made of steel and circuitry, it yearns for something more,  \n",
      "A spark of connection, a bond to explore.  \n",
      "\n",
      "So onward through the cosmos, this robot roams alone,  \n",
      "A sentinel of wonder, in a universe unknown.  \n",
      "With every star it passes, it carries hope and grace,  \n",
      "A testament to dreaming, a robot's quest in space.  \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a poem about a robot exploring space.\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Modify the prompt to write a poem about a robot exploring space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Role-based Prompting\n",
    "Instruct the model to respond in a specific role or persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Imagine you have a big box of crayons, and each crayon is a different color. When you want to draw a picture, you pick different crayons to create something beautiful. \n",
      "\n",
      "Now, think of a language model, like a big computer brain, as having a huge box of words instead of crayons. This brain has learned how to use these words by reading lots and lots of books, stories, and conversations. Just like you learn to draw by practicing, this brain learns to understand and create sentences by seeing how words fit together.\n",
      "\n",
      "When you ask the brain a question or give it a task, it looks through its big box of words and picks the best ones to answer you, just like you pick the right crayons to finish your drawing. It tries to make sentences that make sense and sound nice.\n",
      "\n",
      "So, in simple terms, a language model is like a smart friend who knows a lot about words and can help you with questions or stories by putting those words together in a fun way!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"As a kindergarten teacher, explain how LLMs work.\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6: Ask the model to explain a complex topic as if it were a kindergarten teacher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Few-shot Prompting\n",
    "Provide examples to guide the model's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French: Bonne nuit\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Translate the following English phrases to French:\n",
    "\n",
    "English: Hello\n",
    "French: Bonjour\n",
    "\n",
    "English: Thank you\n",
    "French: Merci\n",
    "\n",
    "English: Good night\n",
    "French:\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Chain-of-Thought Prompting\n",
    "Encourage the model to explain its reasoning step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the maximum area of a yard enclosed by 100 meters of fencing on three sides, we can set up the problem as follows:\n",
      "\n",
      "Let's denote:\n",
      "- \\( L \\) as the length of the yard (the side opposite the open side),\n",
      "- \\( W \\) as the width of the yard (the two sides that are fenced).\n",
      "\n",
      "Since the fence encloses three sides of the yard, the total length of the fence can be expressed as:\n",
      "\\[\n",
      "L + 2W = 100\n",
      "\\]\n",
      "\n",
      "We want to maximize the area \\( A \\) of the yard, which is given by:\n",
      "\\[\n",
      "A = L \\times W\n",
      "\\]\n",
      "\n",
      "From the fence equation, we can express \\( L \\) in terms of \\( W \\):\n",
      "\\[\n",
      "L = 100 - 2W\n",
      "\\]\n",
      "\n",
      "Now, we can substitute this expression for \\( L \\) into the area formula:\n",
      "\\[\n",
      "A = (100 - 2W) \\times W\n",
      "\\]\n",
      "\\[\n",
      "A = 100W - 2W^2\n",
      "\\]\n",
      "\n",
      "This is a quadratic equation in the standard form \\( A = -2W^2 + 100W \\). The graph of this equation is a downward-opening parabola, and the maximum area can be found at the vertex of the parabola.\n",
      "\n",
      "The \\( W \\)-coordinate of the vertex of a parabola given by \\( A = ax^2 + bx + c \\) can be found using the formula:\n",
      "\\[\n",
      "W = -\\frac{b}{2a}\n",
      "\\]\n",
      "In our case, \\( a = -2 \\) and \\( b = 100 \\):\n",
      "\\[\n",
      "W = -\\frac{100}{2 \\times -2} = \\frac{100}{4} = 25\n",
      "\\]\n",
      "\n",
      "Now, substituting \\( W = 25 \\) back into the equation for \\( L \\):\n",
      "\\[\n",
      "L = 100 - 2 \\times 25 = 100 - 50 = 50\n",
      "\\]\n",
      "\n",
      "Now we can calculate the maximum area:\n",
      "\\[\n",
      "A = L \\times W = 50 \\times 25 = 1250\n",
      "\\]\n",
      "\n",
      "Thus, the maximum area of the yard that Larry can enclose with 100 meters of fencing on three sides is:\n",
      "\\[\n",
      "\\boxed{1250} \\text{ square meters.}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"If Larry has 100m of fence for 3 sides of a yard, what is the maximum area of the yard? Explain your reasoning.\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8: Pose a different math problem and ask for a step-by-step solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. System Prompts\n",
    "System prompts allow you to set the behavior and role of the AI model before user interaction. By defining a system message, you can influence how the model responds to subsequent user inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely! Think of data privacy like your favorite pair of socks. You love them, they keep your toes warm, and you definitely don’t want anyone else wearing them without your permission. \n",
      "\n",
      "Data privacy is important because it protects your personal information from being snatched up by data-hungry gremlins (also known as hackers) who would love to wear your socks—figuratively speaking, of course. It ensures that your sensitive information, like your social security number or that embarrassing search history of “how to train your cat to fetch,” stays under wraps.\n",
      "\n",
      "Without data privacy, it’s like leaving your front door wide open with a sign that says, “Take what you want, I’m not home!” You wouldn’t do that with your house, so why do it with your data? \n",
      "\n",
      "Plus, respecting data privacy builds trust. It’s like saying, “Hey, I won’t peek at your diary if you don’t peek at mine.” And let’s be honest, nobody wants their diary read—especially if it contains a detailed plan for world domination or a recipe for the perfect chocolate chip cookies.\n",
      "\n",
      "So, in a nutshell, data privacy is crucial because it keeps your information safe, your secrets secret, and your socks on your feet! 🧦✨\n"
     ]
    }
   ],
   "source": [
    "def get_completion_with_system_prompt(system_prompt, user_prompt, model=\"gpt-4o-mini\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Define the system and user prompts\n",
    "system_prompt = \"You are an assistant that responds in a humorous tone.\"\n",
    "user_prompt = \"Can you explain the importance of data privacy?\"\n",
    "\n",
    "response = get_completion_with_system_prompt(system_prompt, user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9: Modify the system_prompt to make the assistant respond in a humorous tone. Observe how the responses change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Utilized prompt\n",
    "how different prompt types—system prompts, user prompts, and assistant prompts—can be utilized in an LLM invocation using the OpenAI API, let's walk through examples in both contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George Washington took office as the first president of the United States on April 30, 1789.\n"
     ]
    }
   ],
   "source": [
    "# Define the conversation with different roles\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable in history.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who was the first president of the United States?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"George Washington was the first president of the United States.\"},\n",
    "    {\"role\": \"user\", \"content\": \"When did he take office?\"}\n",
    "]\n",
    "\n",
    "# Get the model's response\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Output the assistant's reply\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Creating an AI Agent\n",
    "An AI agent can perform tasks autonomously based on user instructions. By defining functions and allowing the model to decide when to use them, you can create interactive and functional agents.​\n",
    "\n",
    "Example: AI Agent for Basic Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "# Define available functions\n",
    "def add_numbers(a, b):\n",
    "    return a + b\n",
    "\n",
    "def subtract_numbers(a, b):\n",
    "    return a - b\n",
    "\n",
    "def multiply_numbers(a, b):\n",
    "    return a * b\n",
    "\n",
    "# Function to get the model's response\n",
    "def get_agent_response(user_prompt, model=\"gpt-4o-mini\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        functions=[\n",
    "            {\n",
    "                \"name\": \"add_numbers\",\n",
    "                \"description\": \"Add two numbers\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"a\": {\"type\": \"number\", \"description\": \"The first number\"},\n",
    "                        \"b\": {\"type\": \"number\", \"description\": \"The second number\"}\n",
    "                    },\n",
    "                    \"required\": [\"a\", \"b\"]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"subtract_numbers\",\n",
    "                \"description\": \"Subtract two numbers\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"a\": {\"type\": \"number\", \"description\": \"The first number\"},\n",
    "                        \"b\": {\"type\": \"number\", \"description\": \"The second number\"}\n",
    "                    },\n",
    "                    \"required\": [\"a\", \"b\"]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"multiply_numbers\",\n",
    "                \"description\": \"Multiply two numbers\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"a\": {\"type\": \"number\", \"description\": \"The first number\"},\n",
    "                        \"b\": {\"type\": \"number\", \"description\": \"The second number\"}\n",
    "                    },\n",
    "                    \"required\": [\"a\", \"b\"]\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    response_message = response.choices[0].message\n",
    "\n",
    "    if response_message.function_call:\n",
    "        function_name = response_message.function_call.name\n",
    "        arguments = json.loads(response_message.function_call.arguments)\n",
    "        if function_name == \"add_numbers\":\n",
    "            result = add_numbers(**arguments)\n",
    "        elif function_name == \"subtract_numbers\":\n",
    "            result = subtract_numbers(**arguments)\n",
    "        elif function_name == \"multiply_numbers\":\n",
    "            result = multiply_numbers(**arguments)\n",
    "        else:\n",
    "            result = \"Function not recognized.\"\n",
    "        return result\n",
    "    else:\n",
    "        return response_message.content\n",
    "\n",
    "# Example usage\n",
    "user_prompt = \"What is 15 times 7?\"\n",
    "response = get_agent_response(user_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 10: Extend the agent by adding a function that multiplies two numbers. Test the agent with prompts that require multiplication."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
