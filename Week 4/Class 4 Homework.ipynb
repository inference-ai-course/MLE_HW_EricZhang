{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Retrieval-Augmented Generation (RAG) with arXiv Papers\n",
    "This week marks a major shift in your AI agent's capabilities: youâ€™ll build the foundation for a Retrieval-Augmented Generation (RAG) system tailored to scientific research. Rather than relying on an LLMâ€™s memory alone, RAG architectures allow your agent to search a structured knowledge base and generate grounded, document-aware answers.\n",
    "\n",
    "Your task is to create a RAG pipeline using recent arXiv cs.CL papers, converting them into searchable chunks, embedding them, and indexing them with FAISS. Youâ€™ll then implement a simple query interface that takes a user question, retrieves the top relevant chunks, and displays them for further processing.\n",
    "\n",
    "This week marks the beginning of building your agentâ€™s private research knowledge baseâ€”a semantic index that youâ€™ll evolve into a full-featured hybrid database in Week 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "* Understand the components of a Retriever-Reader QA pipeline.\n",
    "* Explore document chunking strategies (e.g., sections vs. sliding windows) and their impact on retrieval performance.\n",
    "* Index scientific text using vector embeddings and FAISS.\n",
    "* Build and query a semantic index via a FastAPI endpoint that returns relevant passages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Design\n",
    "\n",
    "The project will guide you through building a RAG pipeline on arXiv cs.CL papers:\n",
    "\n",
    "1. **Data Collection:** Obtain 50 arXiv cs.CL PDFs (you can scrape via the arXiv API or use a provided sample set).\n",
    "2. **Text Extraction:** Extract raw text from each PDF (for example, using PyMuPDF's `get_text()` on each page). Clean and concatenate the page text into full-document strings.\n",
    "3. **Text Chunking:** Split each paper into chunks (â‰¤ 512 tokens each). You might split at section boundaries or use a sliding-window approach (e.g., 500-token windows with overlap). Chunking into smaller, meaningful segments (around 250â€“512 tokens) often yields better retrieval precision.\n",
    "4. **Embedding Generation:** Compute dense vector embeddings for each chunk. For instance, using the `sentence-transformers` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\OneDrive\\Documents\\Programming\\Inference AI Course\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\zhang\\OneDrive\\Documents\\Programming\\Inference AI Course\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zhang\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'list_of_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(\u001b[43mlist_of_chunks\u001b[49m)  \u001b[38;5;66;03m# embeds each text chunk into a 384-d vecto\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'list_of_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(list_of_chunks)  # embeds each text chunk into a 384-d vecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (Alternatively, you can use a Hugging Face Transformer model and apply pooling manually to get chunk embeddings.)\n",
    "5. **Indexing with FAISS:** Build a FAISS index of the chunk embeddings. For example, use a simple index like `IndexFlatL2` with the same dimensionality as your embeddings. Add all chunk vectors to the index (e.g., `index.add(np.array(embeddings))`).\n",
    "6. **Notebook Demo:** Create a notebook where a user query is embedded and passed to the index (`index.search(query_embedding, k)`) to retrieve the top-3 matching chunks. Display the original chunk text for these results.\n",
    "7. **FastAPI Service:** Build a simple FastAPI app. Define an endpoint (e.g. `@app.get(\"/search\")`) that accepts a query parameter `q`. In the handler, embed `q`, perform the FAISS search, and return the top passages as JSON. (For example, a FastAPI endpoint can accept a question and return relevant documents.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Starter Code Snippets\n",
    "\n",
    "Below are skeleton code templates. Fill in the details (indicated by comments or ellipses).\n",
    "\n",
    "**Data Extraction (PDF â†’ Text):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyMuPDF 1.26.3: Python bindings for the MuPDF 1.26.3 library (rebased implementation).\n",
      "Python 3.9 running on win32 (64-bit).\n",
      "\n",
      "c:\\Users\\zhang\\OneDrive\\Documents\\Programming\\Inference AI Course\\.venv\\lib\\site-packages\\fitz\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Open a PDF and extract all text as a single string.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    for page in doc:\n",
    "        page_text = page.get_text()  # get raw text from page\n",
    "        # (Optional) clean page_text here (remove headers/footers)\n",
    "        pages.append(page_text)\n",
    "    full_text = \"\\n\".join(pages)\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paths = [\"data/2508.15711v1.pdf\", \"data/2508.15721v1.pdf\", \"data/2508.15746v1.pdf\", \"data/2508.15754v1.pdf\", \"data/2508.15760v1.pdf\"]\n",
    "corpus_texts = [extract_text_from_pdf(p) for p in pdf_paths]\n",
    "full_corpus = \"\\n\\n\".join(corpus_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunking Logic (Sliding Window):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 512, overlap: int = 50) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    step = max_tokens - overlap\n",
    "    for i in range(0, len(tokens), step):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_chunks = []\n",
    "for doc in corpus_texts:\n",
    "    list_of_chunks.extend(chunk_text(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\OneDrive\\Documents\\Programming\\Inference AI Course\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(list_of_chunks)  # embeds each text chunk into a 384-d vecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Embedding Generation (Sentence-Transformers):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def chunk_text(text: str, max_tokens: int = 512, overlap: int = 50) -> List[str]:\n",
    "#     tokens = text.split()\n",
    "#     chunks = []\n",
    "#     step = max_tokens - overlap\n",
    "#     for i in range(0, len(tokens), step):\n",
    "#         chunk = tokens[i:i + max_tokens]\n",
    "#         chunks.append(\" \".join(chunk))\n",
    "#     return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**FAISS Indexing and Search:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heng Sia, Chai Rick Soh, Joshua Yi Min Tung, Jasmine Chiat Ling Ong, Chang Fu Kuo, Shao-Chun Wu, Vesela P. Kovacheva, and Daniel Shu Wei Ting. Retrieval augmented generation for 10 large language models and its generalizability in assessing medical fitness. NPJ Digital Medicine, 8, 2025. 1 [7] Lameck Mbangula Amugongo, Pietro Mascheroni, Steven Brooks, Stefan Doering, and Jan Seidel. Retrieval augmented generation for large language models in healthcare: A systematic review. PLOS Digital Health, 4(6):e0000877, 2025. 1 [8] Binxu Li, Tiankai Yan, Yuanting Pan, Jie Luo, Ruiyang Ji, Jiayuan Ding, Zhe Xu, Shilong Liu, Haoyu Dong, Zihao Lin, and Yixin Wang. MMedAgent: Learning to use medical tools with multi-modal agent. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 8745â€“8760, Miami, Florida, USA, November 2024. Association for Computational Linguistics. 1 [9] Shanghua Gao, Richard Zhu, Zhenglun Kong, Ayush Noori, Xiaorui Su, Curtis Ginder, Theodoros Tsiligkaridis, and Marinka Zitnik. Txagent: An ai agent for therapeutic reasoning across a universe of tools. arXiv preprint arXiv:2503.10970, 2025. 1 [10] Simone Kresevic, Mauro GiuffrÃ¨, Milos Ajcevic, Agostino Accardo, Lory S CrocÃ¨, and Dennis L Shung. Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework. NPJ digital medicine, 7(1):102, 2024. 1 [11] Rui Yang, Yilin Ning, Emilia Keppo, Mingxuan Liu, Chuan Hong, Danielle S Bitterman, Jasmine Chiat Ling Ong, Daniel Shu Wei Ting, and Nan Liu. Retrieval-augmented generation for generative artificial intelligence in health care. npj Health Systems, 2(1):2, 2025. 1 [12] Kimberly LeBlanc, Emily Glanton, Anna Nagy, Jorick Bater, Tala Berro, Molly A McGuinness, Courtney Studwell, Undiagnosed Diseases Network, and Matthew Might. Rare disease patient matchmaking: development and outcomes of an internet case-finding strategy in the undiagnosed diseases network. Orphanet journal of rare diseases, 16(1):210, 2021. 1 [13] Yixiang Chen, Penglei Sun, Xiang Li, and Xiaowen Chu. Mrd-rag: enhancing medical diagnosis with multi-round retrieval-augmented generation. arXiv preprint arXiv:2504.07724, 2025. 1 [14] Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Cheng Jiayang, Cunxiang Wang, Shichao Sun, Huanyu Li, et al. Ragchecker: A fine-grained framework for diagnosing retrieval-augmented generation. Advances in Neural Information Processing Systems, 37:21999â€“22027, 2024. 1 |23 [15] Yunfan Gao, Yun Xiong, Yijie Zhong, Yuxi Bi, Ming Xue, and Haofen Wang. Synergizing rag and reasoning: A systematic review. arXiv preprint arXiv:2504.15909, 2025. 1 [16] Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, and Jaewoo Kang. Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models. Bioinformatics, 40(Supplement_1):i119â€“i129, 2024. 1 [17] James L Cross, Michael A Choma, and John A Onofrey. Bias in medical ai: Implications for clinical decision-making. PLOS Digital Health, 3(11):e0000651, 2024. 2 [18] Kristin M Kostick-Quenet and Sara Gerke. Ai in the hands of imperfect users. NPJ digital medicine, 5(1):197, 2022. 2 [19] Byron Crowe and Jorge A Rodriguez. Identifying and addressing bias in artificial intelligence. JAMA Network Open, 7(8):e2425955â€“e2425955, 2024. 2 [20] Qiaoyu Zheng, Chaoyi Wu, Pengcheng Qiu, Lisong Dai, Ya Zhang, Yanfeng Wang, and Weidi Xie. How well can modern llms act as agent cores in radiology environments? arXiv preprint arXiv:2412.09529,\n",
      "was even greater than the improvement we achieved by focusing on optimizing the answer reward process in the third stage. This proves the effectiveness |20 of staged adaptation and the importance of process guidance. 5.3 Baselines We introduce the comparison setting of our agentic RL training approach agains other training and prompting approach, then further compare Deep-DxSearch against seven competing baseline methods including domain- adapted medical LLMs, foundation models, retrieval-augmented methods, and multi-agent frameworks, etc. Basic Training & Prompting Approach â€¢ Vanilla model with direct inference. We only prompt the vanilla model to direct diagnose according to its internal knowledge without any post-training. The medical retrieval corpus is disabled under this setting. The input is free-text clinical presentation and no chain-of-thought inference is implemented. â€¢ Training-free RAG-augmented prompting. For comparison, we also include a prompt engineering based approach using the same retrieval corpus (the LLM can interact with the corpus at any time it decided to do). In this inference-only setting, we apply the same prompt design (see Supplemen- tary Materials) as in our agentic RL training, but without incorporating any reward mechanism for optimization. â€¢ Target-only RL training. In contrast to our agentic RL training approach, this target-only training variant removes the policy reward that guides the optimization of the reasoning and retrieval processes, resulting in supervision based solely on target outputs. For a fair comparison, we adopt the same environment settings and training parameters as in our full-component agentic RL training. Competing Clinical Diagnostic Methods â€¢ General-purpose large language model. In this work, we employ the Qwen2.5 [52] and Llama3.1 [53] series as the vanilla backbones for RL training. Specifically, considering the cost-effect tradeoff, we use Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct, and Llama3.1-8B-Instruct. For larger-scale LLMs as comparison baselines, we adopt GPT-4o (proprietary) [30] and DeepSeek-R1 (open-source) [31]. In particular, we access their official APIs with the models DeepSeek-R1-0528 and gpt-4o-2024-11-20. â€¢ Biomedical CLIP-based encoder. These models are trained on large-scale biomedical text corpora using a contrastive learning approach. In this work, we adopt a representative approach: MedCPT [32] for comparison, treating the clinical presentation as the â€œarticleâ€ and the diagnosis as the â€œquery.â€ Specifically, we use the official Hugging Face checkpoint ncbi/MedCPT-Cross-Encoder. â€¢ Medical large language model. Domain-adaptive pretraining (DAPT) of general LLMs on medi- cal corpora is a common approach for clinical adaptation [54]. In this work, we adopt the newly developed Baichuan-M1 model as a baseline with the official checkpoint baichuan-inc/Baichuan-M1-14B-Instruct. â€¢ Medical foundation model. We include this category as multi-modal, multi-task generalisers. Medical foundation models such as Meditron [55] and MedFound [56] demonstrate strong capabilities across diverse clinical scenarios, including diagnosis. In this work, we select MedGemma [34] for its improved instruction-following ability and more recent medical knowledge cutoff. The official checkpoint used is google/medgemma-27b-text-it. â€¢ Medical RAG-based framework. Different from our retrieval approach, these methods typically rely on a general medical knowledge corpus specified via a system prompt, without fine-tuning. In this work, we include the MedRAG [35] framework, following the official implementation Teddy-XiongGZ/MedRAG. â€¢ Chain-of-Thought agentic model. This type of model incorporates the chain-of-thought paradigm through supervised\n",
      "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning Qiaoyu Zheng1,2, Yuze Sun1, Chaoyi Wu1, Weike Zhao1,2, Pengcheng Qiu1,2, Yongguo Yu3, Kun Sun3, Yanfeng Wang1,2, Ya Zhang1,2,â€  and Weidi Xie1,2,â€  1Shanghai Jiao Tong University, Shanghai, China 2Shanghai AI Laboratory, Shanghai, China 3Xinhua Hospital affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai, China Accurate diagnosis remains a central challenge for medical large language models due to inherent knowledge limitations and hallucinations. While retrieval-augmented generation (RAG) and tool-augmented agentic methods show potential in mitigating these issues, their suboptimal utilization of external knowledge and the decoupling of the feedback-reasoning traceability, stemming from insufficient supervision, re- main key limitations. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crucially, we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL. Experiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, Deep- DxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution (ID) and out-of-distribution (OOD) settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearchâ€™s diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. Data, code, and checkpoints are available at https://github.com/MAGIC-AI4Med/Deep-DxSearch. 1 INTRODUCTION AI-driven medical diagnosis [1] presents unique challenges, as it must replicate the precision and context- awareness of clinical decision-making [2]. Such decision-making is inherently evidence-based, drawing on up-to-date guidelines, historical patient records, and structured medical knowledge to map presenting symptoms to plausible diseases [3, 4]. Recent LLM-based agentic retrieval-augmented generation (RAG) systems [5, 6, 7] have highlighted promising directions for building more powerful LLM-based diagnostic systems. By leveraging the orchestration capabilities of LLMs in conjunction with retrieval tools [8, 9], these systems can look up disease guidelines [10], search for related background knowledge [11], and, most critically for diagnosis, match similar diagnostic cases [12], ultimately synthesizing transparent and traceable diagnostic reasoning interwoven with retrieved evidence and analytical insights. While promising, current agentic RAG system designs are typically inference-only and not trained end-to-end, which makes them fragile in high-stakes diagnostic environments where the agent may need to perform multiple retrievals [13] interleaved with evolving reasoning processes and former noisy retrieval feedback [14]. In particular, they exhibit THREE key limitations: â€¢ Rigid retrievalâ€“reasoning interleaved workflow. Inference-only designs [15, 16] lack joint optimiza- tion, leaving models unable to decide when tools or reasoning should be performed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Assume embeddings is a 2D numpy array of shape (num_chunks, dim)\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)  # using a simple L2 index\n",
    "index.add(np.array(embeddings))  # add all chunk vectors\n",
    "\n",
    "# Example: search for a query embedding\n",
    "query = \"What is retrieval augmented generation?\"\n",
    "query_embedding = model.encode([query]) # get embedding for the query (shape: [1, dim])\n",
    "k = 3\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "# indices[0] holds the top-k chunk indices\n",
    "\n",
    "for i in indices[0]:\n",
    "    print(list_of_chunks[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**FastAPI Route Skeleton:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI\n",
    "import numpy as np\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/search\")\n",
    "async def search(q: str):\n",
    "    \"\"\"\n",
    "    Receive a query 'q', embed it, retrieve top-3 passages, and return them.\n",
    "    \"\"\"\n",
    "    # TODO: Embed the query 'q' using your embedding model\n",
    "    query_vector = ...  # e.g., model.encode([q])[0]\n",
    "    # Perform FAISS search\n",
    "    k = 3\n",
    "    distances, indices = faiss_index.search(np.array([query_vector]), k)\n",
    "    # Retrieve the corresponding chunks (assuming 'chunks' list and 'indices' shape [1, k])\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        results.append(chunks[idx])\n",
    "    return {\"query\": q, \"results\": results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     results \u001b[38;5;241m=\u001b[39m [list_of_chunks[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is retrieval-augmented generation?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(query, top_k)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msearch\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, top_k: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     q_emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mencode([query])\n\u001b[0;32m      3\u001b[0m     distances, indices \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39msearch(np\u001b[38;5;241m.\u001b[39marray(q_emb), top_k)\n\u001b[0;32m      4\u001b[0m     results \u001b[38;5;241m=\u001b[39m [list_of_chunks[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[38;5;241m0\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def search(query: str, top_k: int = 3):\n",
    "    q_emb = model.encode([query])\n",
    "    distances, indices = index.search(np.array(q_emb), top_k)\n",
    "    results = [list_of_chunks[i] for i in indices[0]]\n",
    "    return results\n",
    "\n",
    "print(search(\"What is retrieval-augmented generation?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deliverables\n",
    "\n",
    "* **Code Notebook / Script:** Complete code for the RAG pipeline (PDF extraction, chunking, embedding, indexing, retrieval).\n",
    "* **Data & Index:** The FAISS index file and the set of 50 processed paper chunks (e.g., as JSON or pickled objects).\n",
    "* **Retrieval Report:** A brief report showing at least 5 example queries and the top-3 retrieved passages for each, to demonstrate system performance.\n",
    "* **FastAPI Service:** The FastAPI app code (e.g. `main.py`) and instructions on how to run it. The `/search` endpoint should be demonstrable (e.g. returning top-3 passages in JSON for sample queries).\n",
    "\n",
    "## Student Exploration Tips\n",
    "\n",
    "* Experiment with different chunk sizes and overlaps. Smaller chunks (âˆ¼250 tokens) often give more precise retrieval, while larger chunks include more context.\n",
    "* Try different embedding models (e.g. using `'all-mpnet-base-v2'` or `'paraphrase-MiniLM-L6-v2'`) to see how retrieval results change.\n",
    "* Implement a simple reranking step: for example, after retrieving candidates with FAISS, re-score them with a cross-encoder model for finer ranking.\n",
    "* Use metadata: consider filtering or weighting chunks by paper metadata (e.g. year, authors, keywords) to improve relevance if needed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
